data_text_lines <- utils::capture.output(print(as.data.frame(data), print.gap = 2, row.names = FALSE))
max_data_lines <- 200
if(length(data_text_lines) > max_data_lines) {
data_text_lines <- c(data_text_lines[1:max_data_lines], "...", paste("Dataset truncated after", max_data_lines, "lines for brevity."))
}
full_data_text <- paste(data_text_lines, collapse = "\n")
full_prompt <- paste0(prompt, "\n\nFull dataset for reference (or first ", max_data_lines," lines if large):\n\n```\n", full_data_text, "\n```")
}
api_url <- paste0("https://generativelanguage.googleapis.com/v1beta/models/", connection$model, ":generateContent?key=", connection$api_key)
# Default Gemini parameters, can be overridden by llm_params
default_gemini_params <- list(
temperature = 0.3,
maxOutputTokens = 4096,
topP = 0.95,
topK = 40
)
final_gemini_params <- utils::modifyList(default_gemini_params, llm_params)
# Ensure tokens and K are integers
final_gemini_params$maxOutputTokens <- as.integer(final_gemini_params$maxOutputTokens)
final_gemini_params$topK <- as.integer(final_gemini_params$topK)
body <- list(
contents = list(list(parts = list(list(text = full_prompt)))),
generationConfig = final_gemini_params
)
response <- httr::POST(
url = api_url,
httr::add_headers("Content-Type" = "application/json"),
body = jsonlite::toJSON(body, auto_unbox = TRUE, pretty = FALSE), # pretty=FALSE for production
encode = "json"
)
if (httr::status_code(response) != 200) {
error_content <- httr::content(response, "text", encoding = "UTF-8")
stop(paste("Gemini API request failed (", connection$model, ") with status ", httr::status_code(response), ": ", error_content, sep=""))
}
content <- httr::content(response, "parsed", encoding = "UTF-8")
result_text <- tryCatch({
if (!is.null(content$candidates) && length(content$candidates) > 0 &&
!is.null(content$candidates[[1]]$content) &&
!is.null(content$candidates[[1]]$content$parts) && length(content$candidates[[1]]$content$parts) > 0 &&
!is.null(content$candidates[[1]]$content$parts[[1]]$text)) {
content$candidates[[1]]$content$parts[[1]]$text
} else {
# Check for safety ratings or other reasons for no content
if (!is.null(content$promptFeedback) && !is.null(content$promptFeedback$blockReason)) {
stop(paste0("Gemini API blocked the prompt. Reason: ", content$promptFeedback$blockReason,
". Details: ", content$promptFeedback$blockReasonMessage %||% "N/A"))
} else if (!is.null(content$candidates[[1]]$finishReason) && content$candidates[[1]]$finishReason != "STOP") {
stop(paste0("Gemini API finished with reason: ", content$candidates[[1]]$finishReason,
". Safety ratings: ", jsonlite::toJSON(content$candidates[[1]]$safetyRatings %||% "N/A")))
}
stop("Failed to extract text from Gemini API response: No valid text part found or content blocked.")
}
}, error = function(e) {
stop(paste("Error parsing Gemini API response (", connection$model, "): ", e$message,
# "\nRaw content: ", jsonlite::toJSON(content, auto_unbox=TRUE, pretty=TRUE) # Uncomment for deep debugging
sep=""))
})
return(result_text)
}
# --- OpenAI Specific Implementation (Placeholder) ---
#' Create LLM connection for OpenAI
#' @param provider Ignored.
#' @param api_key Your OpenAI API key.
#' @param model OpenAI model name (e.g., "gpt-4-turbo-preview", "gpt-3.5-turbo").
#' @param organization_id Optional OpenAI organization ID.
#' @param ... Additional arguments.
#' @return An "openai_connection" object.
#' @exportS3method create_llm_connection openai
create_llm_connection.openai <- function(provider, api_key, model, organization_id = NULL, ...) {
if (missing(api_key) || is.null(api_key) || api_key == "") {
stop("OpenAI API key is required.")
}
if (missing(model) || is.null(model) || model == "") {
stop("OpenAI model name is required.")
}
connection <- list(
api_key = api_key,
model = model,
organization_id = organization_id,
provider_name = "openai",
api_base_url = "https://api.openai.com/v1" # Default, can be overridden
)
# Potentially add other args like api_base_url if using Azure OpenAI or proxies
dots <- list(...)
if("api_base_url" %in% names(dots)) connection$api_base_url <- dots$api_base_url
class(connection) <- c("openai_connection", "llm_connection")
return(connection)
}
#' Send prompt to OpenAI API (Placeholder)
#' @param connection An "openai_connection" object.
#' @param prompt Text prompt to send.
#' @param data Optional dataset (typically not sent directly; summarize in prompt).
#' @param llm_params A list of parameters for OpenAI's API,
#'                   e.g., list(temperature = 0.3, max_tokens = 4000, top_p = 1.0,
#'                                system_message = "You are a helpful R programming assistant.").
#' @param ... Additional arguments.
#' @return OpenAI response text.
#' @exportS3method send_to_llm openai_connection
send_to_llm.openai_connection <- function(connection, prompt, data = NULL, max_rows = 50, max_cols = 20,
llm_params = list(), ...) {
# OpenAI typically uses a 'messages' array with roles (system, user, assistant)
# The 'prompt' here would be the user message.
# 'data' should be summarized and included within the user prompt.
# Default OpenAI parameters
default_openai_params <- list(
temperature = 0.3,
max_tokens = 4000, # Adjust based on model
top_p = 1.0,
frequency_penalty = 0,
presence_penalty = 0,
system_message = "You are an expert R statistician and programmer. Provide complete and accurate R code and interpretations as requested."
)
final_openai_params <- utils::modifyList(default_openai_params, llm_params)
messages <- list()
if (!is.null(final_openai_params$system_message) && nzchar(final_openai_params$system_message)) {
messages <- c(messages, list(list(role = "system", content = final_openai_params$system_message)))
}
full_user_prompt <- prompt
if (!is.null(data) && nrow(data) <= max_rows && ncol(data) <= max_cols) {
# Summarize data within the prompt for OpenAI
data_text_lines <- utils::capture.output(print(as.data.frame(data), print.gap = 2, row.names = FALSE))
max_data_lines <- 200
if(length(data_text_lines) > max_data_lines) {
data_text_lines <- c(data_text_lines[1:max_data_lines], "...", paste("Dataset truncated after", max_data_lines, "lines for brevity."))
}
full_data_text <- paste(data_text_lines, collapse = "\n")
full_user_prompt <- paste0(prompt, "\n\nDataset for reference (or first ", max_data_lines," lines if large):\n\n```\n", full_data_text, "\n```")
}
messages <- c(messages, list(list(role = "user", content = full_user_prompt)))
api_url <- paste0(connection$api_base_url, "/chat/completions") # Common endpoint for chat models
body <- list(
model = connection$model,
messages = messages,
temperature = final_openai_params$temperature,
max_tokens = as.integer(final_openai_params$max_tokens),
top_p = final_openai_params$top_p,
frequency_penalty = final_openai_params$frequency_penalty,
presence_penalty = final_openai_params$presence_penalty
# stream = FALSE # Not handling streaming for now
)
auth_header <- httr::add_headers(
`Authorization` = paste("Bearer", connection$api_key),
`Content-Type` = "application/json"
)
if(!is.null(connection$organization_id) && nzchar(connection$organization_id)){
auth_header <- httr::add_headers(auth_header, `OpenAI-Organization` = connection$organization_id)
}
# Placeholder for actual API call using httr::POST
# response <- httr::POST(url = api_url, auth_header, body = body, encode = "json")
# if (httr::status_code(response) != 200) {
#   stop(paste("OpenAI API request failed:", httr::content(response, "text")))
# }
# content <- httr::content(response, "parsed")
# result_text <- content$choices[[1]]$message$content
# --- Placeholder ---
warning(paste("OpenAI provider ('", connection$model,"') is a placeholder. No actual API call made.", sep=""))
result_text <- paste("Placeholder response for OpenAI. Prompt was:\n", full_user_prompt,
"\nParams:\n", jsonlite::toJSON(final_openai_params, auto_unbox=TRUE, pretty=TRUE))
# --- End Placeholder ---
return(result_text)
}
# --- Anthropic (Claude) Specific Implementation (Placeholder) ---
#' Create LLM connection for Anthropic (Claude)
#' @param provider Ignored.
#' @param api_key Your Anthropic API key.
#' @param model Anthropic model name (e.g., "claude-3-opus-20240229").
#' @param ... Additional arguments, e.g., `anthropic_version`.
#' @return An "anthropic_connection" object.
#' @exportS3method create_llm_connection anthropic
create_llm_connection.anthropic <- function(provider, api_key, model, ...) {
# Similar to OpenAI, setup connection object
# model might be e.g. "claude-3-opus-20240229"
# API key is x-api-key
# Header: anthropic-version (e.g., "2023-06-01")
warning("Anthropic provider connection is a placeholder.")
connection <- list(api_key = api_key, model = model, provider_name = "anthropic",
anthropic_version = list(...)$anthropic_version %||% "2023-06-01",
api_base_url = "https://api.anthropic.com/v1")
class(connection) <- c("anthropic_connection", "llm_connection")
return(connection)
}
#' Send prompt to Anthropic (Claude) API (Placeholder)
#' @param connection An "anthropic_connection" object.
#' @param prompt Text prompt to send.
#' @param data Optional dataset.
#' @param llm_params A list of parameters for Anthropic's API.
#' @param ... Additional arguments.
#' @return Anthropic response text.
#' @exportS3method send_to_llm anthropic_connection
send_to_llm.anthropic_connection <- function(connection, prompt, data = NULL, max_rows = 50, max_cols = 20,
llm_params = list(), ...) {
# Similar to OpenAI, construct messages array. Claude also uses system prompts.
# API endpoint: /messages
# Body: model, system (optional), messages, max_tokens, temperature, etc.
warning(paste("Anthropic (Claude) provider ('", connection$model,"') is a placeholder. No actual API call made.", sep=""))
result_text <- paste("Placeholder response for Anthropic. Prompt was:\n", prompt)
return(result_text)
}
# --- Other LLM Providers (Grok, Qwen, DeepSeek - Placeholders) ---
# You would add similar create_llm_connection.<provider_name> and send_to_llm.<provider_name>_connection methods.
# Each will require understanding their specific API authentication, request body, and response structure.
#' Default method for creating LLM connections
#' @param provider The LLM provider.
#' @param ... Additional arguments.
#' @exportS3method create_llm_connection default
create_llm_connection.default <- function(provider, api_key, model, ...) {
stop(paste("LLM provider '", provider, "' is not supported or a specific create_llm_connection method is missing.", sep=""))
}
#' Default method for sending prompts to LLMs
#' @param connection An LLM connection object.
#' @param ... Additional arguments.
#' @exportS3method send_to_llm default
send_to_llm.default <- function(connection, prompt, ...) {
stop(paste("No specific send_to_llm method implemented for connection class:", class(connection)[1]))
}
# --- Utility functions for code extraction (remain largely the same) ---
#' Enhanced function to extract R code blocks from text
#' @param text The text possibly containing R code blocks.
#' @return A single string containing all extracted R code, or the original text if no blocks are found.
#' @keywords internal
extract_code_blocks <- function(text) {
if (is.null(text) || nchar(trimws(text)) == 0) return("")
pattern_r_explicit <- "```(?:[rR]|\\{[rR]\\})[\\s\\S]*?\\n([\\s\\S]*?)```"
pattern_generic <- "```[\\s\\S]*?\\n([\\s\\S]*?)```"
extracted_code_list <- character(0)
matches_r <- gregexpr(pattern_r_explicit, text, perl = TRUE)
if (matches_r[[1]][1] != -1) {
code_segments <- regmatches(text, matches_r)[[1]]
for (segment in code_segments) {
actual_code <- sub(pattern_r_explicit, "\\1", segment, perl = TRUE)
extracted_code_list <- c(extracted_code_list, actual_code)
}
if (length(extracted_code_list) == 0) {
matches_generic <- gregexpr(pattern_generic, text, perl = TRUE)
if (matches_generic[[1]][1] != -1) {
code_segments <- regmatches(text, matches_generic)[[1]]
for (segment in code_segments) {
actual_code <- sub(pattern_generic, "\\1", segment, perl = TRUE)
extracted_code_list <- c(extracted_code_list, actual_code)
}
if (length(extracted_code_list) > 0) {
return(paste(extracted_code_list, collapse = "\n\n"))
}
return("")
}
#' Process LLM output code to make it more executable.
#' @param code A single string potentially containing R code and surrounding text.
#' @return A cleaner string focusing on the R code.
#' @keywords internal
process_gemini_code <- function(code) { # Consider renaming to process_llm_code if it becomes generic
if (is.null(code) || nchar(trimws(code)) == 0) return("")
lines <- strsplit(code, "\n")[[1]]
non_empty_indices <- which(trimws(lines) != "")
if (length(non_empty_indices) == 0) return("")
lines <- lines[min(non_empty_indices):max(non_empty_indices)]
r_start_patterns <- c(
"^\\s*#", "^\\s*library\\(", "^\\s*require\\(", "^\\s*install\\.packages\\(",
"^\\s*[a-zA-Z0-9_\\.]+\\s*<-", "^\\s*[a-zA-Z0-9_\\.]+\\s*=",
"^\\s*if\\s*\\(", "^\\s*for\\s*\\(", "^\\s*while\\s*\\(", "^\\s*function\\s*\\(",
"^\\s*source\\s*\\(", "^\\s*data\\s*\\(", "^\\s*ggplot\\(", "^\\s*plot\\(", "^\\s*par\\("
)
start_idx <- 1
for (i in 1:length(lines)) {
if (any(sapply(r_start_patterns, function(p) grepl(p, lines[i])))) {
start_idx <- i; break
}
code_lines <- if (start_idx <= length(lines)) lines[start_idx:length(lines)] else character(0)
if (length(code_lines) == 0) return("")
return(paste(code_lines, collapse = "\n"))
}
# Helper for null-coalescing
`%||%` <- function(a, b) if (is.null(a) || length(a) == 0) b else a
document()
rm(list = c("create_llm_connection", "send_to_llm"))
load_all()
document()
install.packages("roxygen2")
library(roxygen2)
library(devtools)
document()
# gemini-2.0-flash: "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
# Simple test
test_data <- data.frame(
x = rnorm(50),
y = rnorm(50),
group = sample(LETTERS[1:3], 50, replace = TRUE)
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
install.packages("roxygen2")
packageVersion("roxygen2") # Verify it's >= 7.0.0
devtools::document()
# gemini-2.0-flash: "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
# Simple test
test_data <- data.frame(
x = rnorm(50),
y = rnorm(50),
group = sample(LETTERS[1:3], 50, replace = TRUE)
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
document()
library(devtools)
document()
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_provider = "gemini",  # <-- Use "gemini" instead of the default
llm_model = "gemini-2.0-flash",  # <-- Keep the full model name here
output_file = "test_analysis.html"
)
document()
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",  # <-- Keep the full model name here
output_file = "test_analysis.html"
)
# gemini-2.0-flash: "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
# Simple test
test_data <- data.frame(
x = rnorm(50),
y = rnorm(50),
group = sample(LETTERS[1:3], 50, replace = TRUE)
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
document()
# gemini-2.0-flash: "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
# Simple test
test_data <- data.frame(
x = rnorm(50),
y = rnorm(50),
group = sample(LETTERS[1:3], 50, replace = TRUE)
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
document()
# gemini-2.0-flash: "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
# Simple test
test_data <- data.frame(
x = rnorm(50),
y = rnorm(50),
group = sample(LETTERS[1:3], 50, replace = TRUE)
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
# gemini-2.0-flash: "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
# Simple test
test_data <- data.frame(
x = rnorm(50),
y = rnorm(50),
group = sample(LETTERS[1:3], 50, replace = TRUE)
)
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
data = test_data
question = "How does y relate to x and does it vary by group?"
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
llm_model = "gemini-2.0-flash"
output_file = "test_analysis.html"
if (is.null(llm_model)) {
llm_model <- switch(llm_provider,
"gemini" = "gemini-2.0-flash",
"openai" = "gpt-3.5-turbo",
"anthropic" = "claude-3-opus-20240229",
"gemini-2.0-flash") # Default to gemini-1.0-pro if provider not recognized
}
is.null(llm_model)
llm_model
connection_args <- c(list(provider = llm_provider, api_key = api_key, model = llm_model), llm_provider_args)
llm_provider = "gemini"
connection_args <- c(list(provider = llm_provider, api_key = api_key, model = llm_model), llm_provider_args)
llm_provider_args = list()
llm_generation_params = list(temperature = 0.3, maxOutputTokens = 4096)
connection_args <- c(list(provider = llm_provider, api_key = api_key, model = llm_model), llm_provider_args)
connection <- do.call(create_llm_connection, connection_args)
connection_args
do.call(create_llm_connection, connection_args)
create_llm_connection()
llm_provider
do.call(create_llm_connection, connection_args)
connection_args
create_llm_connection("genimi", api_key, model="genimi")
create_llm_connection("gemini", api_key, model="gemini-1.0-pro")
create_llm_connection("gemini", api_key, model="gemini-2.0-flash")
evtools::document()
devtools::document()
devtools::load_all()
create_llm_connection("gemini", api_key, model="gemini-2.0-flash")
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk",
llm_model = "gemini-2.0-flash",
output_file = "test_analysis.html"
)
?auto_stat_two_step
api_key = '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'
llm_provider = "openai"
llm_model = "gpt-4"
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
llm_provider = llm_provider,
api_key = api_key,
llm_model = llm_model,
output_file = "test_analysis.html"
)
document()
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
llm_model = "gemini-2.0-flash"
llm_provider = "gemini"
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
llm_provider = llm_provider,
api_key = api_key,
llm_model = llm_model,
output_file = "test_analysis.html"
)
api_key = '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'
llm_provider = "openai"
llm_model = "gpt-4"
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
llm_provider = llm_provider,
api_key = api_key,
llm_model = llm_model,
output_file = "test_analysis.html"
)
document()
api_key = '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'
llm_provider = "openai"
llm_model = "gpt-4"
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
llm_provider = llm_provider,
api_key = api_key,
llm_model = llm_model,
output_file = "test_analysis.html"
)
api_key = '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'  # Your actual Anthropic API key
llm_provider = "anthropic"          # Use "anthropic" as the provider name
llm_model = "claude-3-5-sonnet-20240620"  # For Claude 3.5 Sonnet
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
llm_provider = llm_provider,
api_key = api_key,
llm_model = llm_model,
output_file = "test_analysis.html"
)
document()
api_key = "AIzaSyDfkC8qhC39pm-idkIOwC7f20MYaejd3gk"
llm_model = "gemini-2.0-flash"
llm_provider = "gemini"
# api_key = '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'
# llm_provider = "openai"
# llm_model = "gpt-4"
# api_key = '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'  # Your actual Anthropic API key
# llm_provider = "anthropic"          # Use "anthropic" as the provider name
# llm_model = "claude-3-5-sonnet-20240620"  # For Claude 3.5 Sonnet
result <- auto_stat_two_step(
data = test_data,
question = "How does y relate to x and does it vary by group?",
llm_provider = llm_provider,
api_key = api_key,
llm_model = llm_model,
output_file = "test_analysis.html"
)
